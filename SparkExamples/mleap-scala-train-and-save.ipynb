{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Pipelines - Lending Club Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Lending Club data set, this demo will demonstrate how to:\n",
    "\n",
    "1. Load the research dataset from s3\n",
    "2. Perform feature extraction using Spark ML APIs\n",
    "3. Train a Logistic Regression and a Random Forest Classifier\n",
    "4. Perform Hyperparameter tuning using ParamGrid, Binary Classification Evaluator, and a Cross Validator\n",
    "5. Export the best pipeline (feature transformers and LR/RF models) to an MLeap Bundle, which we'll use to deploy the pipeline to an RESTful API service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{u'executorCores': 2, u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.hadoop:hadoop-aws:2.7.3,ml.combust.mleap:mleap-spark_2.11:0.8.1,com.databricks:spark-avro_2.11:3.0.1', u'spark.executorEnv.JAVA_HOME': u'/usr/lib/jvm/java-8-oracle/jre', u'spark.yarn.appMasterEnv.JAVA_HOME': u'/usr/lib/jvm/java-8-oracle/jre', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}, u'driverMemory': u'2048M'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"kind\": \"spark\",\n",
    "\"driverMemory\": \"2048M\",\n",
    "\"executorCores\": 2,\n",
    " \"conf\":{\"spark.jars.packages\":\"org.apache.hadoop:hadoop-aws:2.7.3,ml.combust.mleap:mleap-spark_2.11:0.8.1,com.databricks:spark-avro_2.11:3.0.1\",\n",
    "         \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n",
    "        \"spark.yarn.appMasterEnv.JAVA_HOME\": \"/usr/lib/jvm/java-8-oracle/jre\",\n",
    "        \"spark.executorEnv.JAVA_HOME\": \"/usr/lib/jvm/java-8-oracle/jre\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Spark ML Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1500400165929_0592</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-172-31-25-53.us-west-2.compute.internal:8090/proxy/application_1500400165929_0592/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-172-31-25-53.us-west-2.compute.internal:8044/node/containerlogs/container_e22_1500400165929_0592_01_000001/harry\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import ml.combust.bundle.serializer.SerializationFormat"
     ]
    }
   ],
   "source": [
    "// Spark Training Pipeline Libraries\n",
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.feature.{StandardScaler, StringIndexer, VectorAssembler, PolynomialExpansion}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier, LogisticRegression, RandomForestClassificationModel}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel, Transformer, PipelineStage}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator}\n",
    "import com.databricks.spark.avro._\n",
    "\n",
    "// MLeap/Bundle.ML Serialization Libraries\n",
    "import ml.combust.mleap.spark.SparkSupport._\n",
    "import resource._\n",
    "import ml.combust.bundle.BundleFile\n",
    "import org.apache.spark.ml.bundle.SparkBundleContext\n",
    "import ml.combust.bundle.serializer.SerializationFormat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import and Explore the Research Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\r\n",
      "-rwxr-xr-x   3 5000 5000    4123652 2017-06-19 16:32 /data/FL_insurance_sample.csv\r\n",
      "-rwxr-xr-x   3 5004 5004  102036996 2017-07-18 22:36 /data/LC_data.csv\r\n",
      "-rwxr-xr-x   3 5000 5000   12527920 2017-07-10 19:58 /data/avro-tools-1.7.7.jar\r\n",
      "drwxr-xr-x   - 5000 5000          1 2017-07-10 18:37 /data/lc_2017060401\r\n",
      "drwxr-xr-x   - 5004 5004          2 2017-07-19 23:37 /data/lc_logit\r\n",
      "-rwxr-xr-x   3 5000 5000        508 2017-07-10 18:30 /data/lending_club_2017060401.avsc\r\n",
      "-rwxr-xr-x   3 5000 5000   48379871 2017-07-10 18:17 /data/lending_club_20170617.avro\r\n",
      "-rwxr-xr-x   3 5000 5000        508 2017-07-10 18:31 /data/lending_club_20170617.avsc\r\n",
      "-rwxr-xr-x   3 5003 5003   28697490 2017-07-10 19:50 /data/modeldb-scala-client.jar\r\n",
      "drwxr-xr-x   - 5003 5003          2 2017-12-05 20:36 /data/rf-model\r\n",
      "-rwxr-xr-x   3 5003 5003     104736 2017-12-05 20:36 /data/sample_libsvm_data.txt\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "\n",
    "! hadoop fs -ls /data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total N Records: 534847"
     ]
    }
   ],
   "source": [
    "//val filePath = \"s3a://mleap-demo/datasources/lending_club_2017060401.avro\"\n",
    "// val filePath = \"hdfs:///data/lending_club_2017060401.avro\"\n",
    "// val dataset = spark.read.avro(filePath)\n",
    "// val dataset = spark.table(\"default.lendingclub2017060401\")\n",
    "\n",
    "val filePath = \"hdfs:///data/LC_data.csv\"\n",
    "// val dataset = spark.read.csv(filePath)\n",
    "\n",
    "val dataset = spark.sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(filePath)\n",
    "\n",
    "println(\"Total N Records: \" + dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total N Approved Records: 109584"
     ]
    }
   ],
   "source": [
    "println(\"Total N Approved Records: \" + dataset.filter(\"default7 == 1.0\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+-----+-----------+--------+----------+--------+---------------+---------------+-----------+-------------------+--------------------------+-------------------+------------------------+-------------+----------------------+---------------+--------------+-------------+------------------------+----------------------+----------------+---------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------+--------------------+------------------+-------------------+---------+\n",
      "|loan_amnt|int_rate|annual_inc|  dti|delinq_2yrs|open_acc|revol_util|default7|term_ 36 months|term_ 60 months|purpose_car|purpose_credit_card|purpose_debt_consolidation|purpose_educational|purpose_home_improvement|purpose_house|purpose_major_purchase|purpose_medical|purpose_moving|purpose_other|purpose_renewable_energy|purpose_small_business|purpose_vacation|purpose_wedding|addr_state_AK|addr_state_AL|addr_state_AR|addr_state_AZ|addr_state_CA|addr_state_CO|addr_state_CT|addr_state_DC|addr_state_DE|addr_state_FL|addr_state_GA|addr_state_HI|addr_state_IA|addr_state_ID|addr_state_IL|addr_state_IN|addr_state_KS|addr_state_KY|addr_state_LA|addr_state_MA|addr_state_MD|addr_state_ME|addr_state_MI|addr_state_MN|addr_state_MO|addr_state_MS|addr_state_MT|addr_state_NC|addr_state_ND|addr_state_NE|addr_state_NH|addr_state_NJ|addr_state_NM|addr_state_NV|addr_state_NY|addr_state_OH|addr_state_OK|addr_state_OR|addr_state_PA|addr_state_RI|addr_state_SC|addr_state_SD|addr_state_TN|addr_state_TX|addr_state_UT|addr_state_VA|addr_state_VT|addr_state_WA|addr_state_WI|addr_state_WV|addr_state_WY|home_ownership_ANY|home_ownership_MORTGAGE|home_ownership_NONE|home_ownership_OTHER|home_ownership_OWN|home_ownership_RENT|default81|\n",
      "+---------+--------+----------+-----+-----------+--------+----------+--------+---------------+---------------+-----------+-------------------+--------------------------+-------------------+------------------------+-------------+----------------------+---------------+--------------+-------------+------------------------+----------------------+----------------+---------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------+--------------------+------------------+-------------------+---------+\n",
      "|   5000.0|   10.65|   24000.0|27.65|        0.0|     3.0|      83.7|       0|              1|              0|          0|                  1|                         0|                  0|                       0|            0|                     0|              0|             0|            0|                       0|                     0|               0|              0|            0|            0|            0|            1|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|            0|                 0|                      0|                  0|                   0|                 0|                  1|        0|\n",
      "+---------+--------+----------+-----+-----------+--------+----------+--------+---------------+---------------+-----------+-------------------+--------------------------+-------------------+------------------------+-------------+----------------------+---------------+--------------+-------------+------------------------+----------------------+----------------+---------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+------------------+-----------------------+-------------------+--------------------+------------------+-------------------+---------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// View the individual records\n",
    "// dataset.select(\"loan_amount\", \"fico_score_group_fnl\", \"dti\", \"emp_length\", \"state\", \"approved\", \"loan_title\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features and filter nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allFeatures: Array[String] = Array(loan_amnt, dti, int_rate, annual_inc, delinq_2yrs, term_ 36 months, term_ 60 months, purpose_car, purpose_credit_card, purpose_debt_consolidation, purpose_educational)"
     ]
    }
   ],
   "source": [
    "// Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "val continuousFeatures = Array(\"loan_amnt\", \"dti\", \"int_rate\",\"annual_inc\")\n",
    "\n",
    "val categoricalFeatures = Array(\"delinq_2yrs\",\n",
    "                                  \"term_ 36 months\",\n",
    "                                  \"term_ 60 months\",\n",
    "                                  \"purpose_car\", \"purpose_credit_card\", \n",
    "                                 \"purpose_debt_consolidation\", \"purpose_educational\")\n",
    "\n",
    "// feature set\n",
    "val allFeatures = continuousFeatures.union(categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasetFiltered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_amnt: double, dti: double ... 10 more fields]"
     ]
    }
   ],
   "source": [
    "// Subset dataset to a handful of features\n",
    "val allCols = allFeatures.union(Seq(\"default7\")).map(dataset.col)\n",
    "val datasetFiltered = dataset.select(allCols: _*).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDataset: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_amnt: double, dti: double ... 10 more fields]\n",
      "validationDataset: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_amnt: double, dti: double ... 10 more fields]"
     ]
    }
   ],
   "source": [
    "val Array(trainingDataset, validationDataset) = datasetFiltered.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipelines\n",
    "\n",
    "Pipeline 1: Scale Features\n",
    "    - VectorAssembler\n",
    "    - StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuousFeatureScaler: org.apache.spark.ml.feature.StandardScaler = continuous_feature_scaler"
     ]
    }
   ],
   "source": [
    "// Pipeline 1\n",
    "val continuousFeatureAssembler = new VectorAssembler(uid = \"continuous_feature_assembler\").\n",
    "    setInputCols(continuousFeatures).\n",
    "    setOutputCol(\"unscaled_continuous_features\")\n",
    "\n",
    "val continuousFeatureScaler = new StandardScaler(uid = \"continuous_feature_scaler\").\n",
    "    setInputCol(\"unscaled_continuous_features\").\n",
    "    setOutputCol(\"scaled_continuous_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline"
     ]
    }
   ],
   "source": [
    "// Assemble Feature Vector For Random Forest Classifier\n",
    "val featureColsRf = allFeatures //continuousFeatureAssembler.map(_.getOutputCol).union(Seq(\"scaled_continuous_features\"))\n",
    "\n",
    "val featureAssemblerRf = new VectorAssembler(uid = \"feature_assembler_rf\").\n",
    "    setInputCols(featureColsRf).\n",
    "    setOutputCol(\"features_rf\")\n",
    "\n",
    "// Define an array of all of our feature estimators that we need to fit\n",
    "val estimators: Array[PipelineStage] = Array(continuousFeatureAssembler, continuousFeatureScaler).\n",
    "    union(Seq(featureAssemblerRf))\n",
    "\n",
    "// Build our pipeline based on the estimators\n",
    "val featurePipeline = new Pipeline(uid = \"feature_pipeline\").\n",
    "    setStages(estimators)\n",
    "\n",
    "// Fit our pipeline\n",
    "val sparkFeaturePipelineModel = featurePipeline.fit(trainingDataset)\n",
    "\n",
    "println(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfWithFeatures: org.apache.spark.sql.DataFrame = [loan_amnt: double, dti: double ... 13 more fields]"
     ]
    }
   ],
   "source": [
    "// apply pipeline to the training set to get final modeling dataset\n",
    "val dfWithFeatures = sparkFeaturePipelineModel.transform(trainingDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res36: Array[org.apache.spark.sql.Row] = Array([(11,[0,1,2,3,4,5],[500.0,3.04,10.71,7904.04,1.0,1.0])])"
     ]
    }
   ],
   "source": [
    "dfWithFeatures.select(\"features_rf\").head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Spark ML Training and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to train a Logistic Regression model and a Random Forest Classifier model on the training data and test their accuracy on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Random Forest"
     ]
    }
   ],
   "source": [
    "// Step 3.1 Create our random forest model\n",
    "val randomForest = new RandomForestClassifier(uid = \"random_forest_classifier\").\n",
    "    setFeaturesCol(\"features_rf\").\n",
    "    setLabelCol(\"default7\").\n",
    "    setPredictionCol(\"approved_prediction\").\n",
    "    setNumTrees(5).\n",
    "    setMaxDepth(5)\n",
    "\n",
    "\n",
    "println(\"Complete: Training Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------+----------+-----------+---------------+---------------+-----------+-------------------+--------------------------+-------------------+--------+\n",
      "|loan_amnt| dti|int_rate|annual_inc|delinq_2yrs|term_ 36 months|term_ 60 months|purpose_car|purpose_credit_card|purpose_debt_consolidation|purpose_educational|default7|\n",
      "+---------+----+--------+----------+-----------+---------------+---------------+-----------+-------------------+--------------------------+-------------------+--------+\n",
      "|    500.0|3.04|   10.71|   7904.04|        1.0|              1|              0|          0|                  0|                         0|                  0|       0|\n",
      "+---------+----+--------+----------+-----------+---------------+---------------+-----------+-------------------+--------------------------+-------------------+--------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "trainingDataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkPipelineRf: org.apache.spark.ml.PipelineModel = pipeline_0fadea4be96f"
     ]
    }
   ],
   "source": [
    "val sparkPipelineEstimatorRf = new Pipeline().setStages(Array(sparkFeaturePipelineModel, randomForest))\n",
    "\n",
    "// fit the random forest piple here: first stage is feature pipleline, then random forest\n",
    "val sparkPipelineRf = sparkPipelineEstimatorRf.fit(trainingDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res44: String =\n",
      "\"RandomForestClassificationModel (uid=rfc_a58db9bf0c70) with 5 trees\n",
      "  Tree 0 (weight 1.0):\n",
      "    If (feature 1 <= 21.24)\n",
      "     If (feature 6 <= 0.0)\n",
      "      If (feature 2 <= 12.29)\n",
      "       If (feature 2 <= 8.94)\n",
      "        If (feature 3 <= 51000.0)\n",
      "         Predict: 0.0\n",
      "        Else (feature 3 > 51000.0)\n",
      "         Predict: 0.0\n",
      "       Else (feature 2 > 8.94)\n",
      "        If (feature 2 <= 11.12)\n",
      "         Predict: 0.0\n",
      "        Else (feature 2 > 11.12)\n",
      "         Predict: 0.0\n",
      "      Else (feature 2 > 12.29)\n",
      "       If (feature 3 <= 43000.0)\n",
      "        If (feature 2 <= 15.59)\n",
      "         Predict: 0.0\n",
      "        Else (feature 2 > 15.59)\n",
      "         Predict: 0.0\n",
      "       Else (feature 3 > 43000.0)\n",
      "        If (feature 2 <= 16.29)\n",
      "         Predict: 0.0\n",
      "        Else (feature 2 > 16.29)\n",
      "         Predict: 0.0\n",
      "     ..."
     ]
    }
   ],
   "source": [
    "val randomForestModel = randomForest.fit(dfWithFeatures)\n",
    "randomForestModel.toDebugString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res45: org.apache.spark.ml.param.ParamMap =\n",
      "{\n",
      "\trfc_a58db9bf0c70-cacheNodeIds: false,\n",
      "\trfc_a58db9bf0c70-checkpointInterval: 10,\n",
      "\trfc_a58db9bf0c70-featureSubsetStrategy: auto,\n",
      "\trfc_a58db9bf0c70-featuresCol: features_rf,\n",
      "\trfc_a58db9bf0c70-impurity: gini,\n",
      "\trfc_a58db9bf0c70-labelCol: default7,\n",
      "\trfc_a58db9bf0c70-maxBins: 32,\n",
      "\trfc_a58db9bf0c70-maxDepth: 5,\n",
      "\trfc_a58db9bf0c70-maxMemoryInMB: 256,\n",
      "\trfc_a58db9bf0c70-minInfoGain: 0.0,\n",
      "\trfc_a58db9bf0c70-minInstancesPerNode: 1,\n",
      "\trfc_a58db9bf0c70-numTrees: 5,\n",
      "\trfc_a58db9bf0c70-predictionCol: approved_prediction,\n",
      "\trfc_a58db9bf0c70-probabilityCol: probability,\n",
      "\trfc_a58db9bf0c70-rawPredictionCol: rawPrediction,\n",
      "\trfc_a58db9bf0c70-seed: 207336481,\n",
      "\trfc_a58db9bf0c70-subsamplingRate: 1.0\n",
      "}"
     ]
    }
   ],
   "source": [
    "randomForestModel.extractParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Validate the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validationDataWithPrediction: org.apache.spark.sql.DataFrame = [loan_amnt: double, dti: double ... 16 more fields]"
     ]
    }
   ],
   "source": [
    "// Run the feature transformers and score the validation dataset with RF model\n",
    "val validationDataWithPrediction = sparkPipelineRf.transform(validationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6964259045202464"
     ]
    }
   ],
   "source": [
    "// Compute the Area Under the Receiver Operating Characteristics (ROC) Curve\n",
    "val evaluator = new BinaryClassificationEvaluator().\n",
    "    setLabelCol(randomForest.getLabelCol).\n",
    "    setRawPredictionCol(randomForest.getRawPredictionCol).\n",
    "    setMetricName(\"areaUnderROC\")\n",
    "    \n",
    "val accuracy = evaluator.evaluate(validationDataWithPrediction)\n",
    "println(\"Accuracy: \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Serializing ML Pipelines for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ml.combust.bundle.BundleFile"
     ]
    }
   ],
   "source": [
    "import ml.combust.bundle.serializer.SerializationFormat\n",
    "import ml.combust.bundle.BundleFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res50: String = /tmp/hadoop-mapr/nm-local-dir/usercache/harry/appcache/application_1500400165929_0592/container_e22_1500400165929_0592_01_000001/."
     ]
    }
   ],
   "source": [
    "import java.io._\n",
    "val sbcRf = SparkBundleContext().withDataset(sparkPipelineRf.transform(datasetFiltered))\n",
    "val localpwd = new File(\".\").getAbsolutePath()\n",
    "localpwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save model to Spark Master node\n",
    "for(bf <- managed(BundleFile(s\"jar:file:$localpwd/lc_pipeline_rf2.zip\"))) {\n",
    "        sparkPipelineRf.writeBundle.save(bf)(sbcRf).get\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res54: String = \"\""
     ]
    }
   ],
   "source": [
    "// Copy file to hadoop fs\n",
    "import sys.process._\n",
    "val hadoopresult = s\"hadoop fs -copyFromLocal -f $localpwd/lc_pipeline_rf2.zip /tmp/harry_lc_pipeline_rf2.zip\" !!\n",
    "hadoopresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "-rwxr-xr-x   3 5003 5003  102036996 2017-12-04 23:24 /tmp/LC_data.csv\n",
      "drwxr-xr-x   - 5005 5005          5 2017-09-15 00:04 /tmp/affinity_df\n",
      "drwxr-xr-x   - 5005 5005          5 2017-09-15 00:04 /tmp/affinity_df_spark_csv\n",
      "-rwxr-xr-x   3 5005 5005       9096 2017-09-13 02:12 /tmp/artist_alias_small.txt\n",
      "-rwxr-xr-x   3 5005 5005     728192 2017-09-13 02:24 /tmp/artist_data_small.txt\n",
      "-rwxr-xr-x   3 5007 5007      11140 2018-01-11 07:05 /tmp/harry_lc_pipeline_rf2.zip\n",
      "drwxr-xr-x   - 5005 5005          2 2017-09-14 23:27 /tmp/iris_tbl_tmp\n",
      "-rwxr-xr-x   3 5005 5005     898234 2017-09-13 02:24 /tmp/user_artist_data_small.txt\n",
      "get: `/home/datascience/loan-risk-demo/SparkExamples/harry_lc_pipeline_rf2.zip': File exists\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Copy file to DS Platform Container\n",
    "\n",
    "! hadoop fs -ls /tmp\n",
    "! hadoop fs -get /tmp/harry_lc_pipeline_rf2.zip /home/datascience/loan-risk-demo/SparkExamples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme.md\t harry_lc_pipeline_rf2.zip\r\n",
      "deploy-scala.py  mleap-scala-train-and-save.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "!ls /home/datascience/loan-risk-demo/SparkExamples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
